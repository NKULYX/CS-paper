# CS paper

## CV

1. AlexNet : *ImageNet Classification with Deep Convolutional Neural Networks*

   [https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf]

2. VggNet : *Very Deep Convolutional Networks for Large-scale Image Recognition*

   [https://arxiv.org/pdf/1409.1556.pdf]

3. ResNet : *Deep Residual Learning for Image Recognition*

   [https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf]

4. *A Neural Algorithm of Artistic Style*

   [https://arxiv.org/pdf/1508.06576.pdf]

5. *Perceptual Losses for Real-Time Style Transfer and Super-Resolution*

   [https://link.springer.com/content/pdf/10.1007/978-3-319-46475-6_43.pdf]

6. *Real-Time High-Resolution Background Matting*

   [http://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Real-Time_High-Resolution_Background_Matting_CVPR_2021_paper.pdf]

7. ViT : *An Image is Worth 16x16 Words : Transformers for Image Recognition at Scale*

   [https://arxiv.org/pdf/2010.11929.pdf]

8. MAE : *Masked Autoencoders Are Scalable Vision Learners*

   [https://arxiv.org/pdf/2111.06377.pdf]
   
8. *Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion*

   [https://arxiv.org/pdf/2205.02357.pdf]

## NLP

1. Transformer : *Attention Is All You Need*

   [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf]

2. BERT : *Pre-training of Deep Bidirectional Transformers for Language Understanding*

   [https://arxiv.org/pdf/1810.04805.pdf]

3. GPT : *Improving Language Understanding by Generative Pre-Training*

   [https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf]
   
3. GPT-2 : *Language Models are Unsupervised Multitask Learners*

   [https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf]
   
3. GPT-3 : *Language Models are Few-Shot Learners*

   [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf]
   
3. *Mining Error Templates for Grammatical Error Correction*

   [https://arxiv.org/pdf/2206.11569.pdf]

## MM

1. CLIP : *Learning Transferable Visual Models From Natural Language Supervision*

   [http://proceedings.mlr.press/v139/radford21a/radford21a.pdf]

2. ViLT : *Vision-and-Language Transformer Without Convolution or Region Supervision*

   [http://proceedings.mlr.press/v139/kim21k/kim21k.pdf]

3. FL-MSRE : *A Few-Shot Learning based Approach to Multimodal Social Relation Extraction*

   [https://ojs.aaai.org/index.php/AAAI/article/view/17639/17446]

## FSL

1. Siamese Network : *Siamese Neural Networks for One-shot Image Recognition*

   [http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf]

2. Matching Network : *Memory Matching Networks for One-Shot Image Recognition*

   [https://arxiv.org/pdf/1804.08281.pdf]

3. Prototype Network : *Prototypical Networks for Few-shot Learning*

   [https://proceedings.neurips.cc/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf]

4. Relation Network ï¼š *Learning to Compare: Relation Network for Few-Shot Learning*

   [https://arxiv.org/pdf/1711.06025.pdf]

## DM

1. Nearest Neighbor Classifiers over Incomplete Information : From Certain Answers to Certain Predictions

   [https://arxiv.org/pdf/2005.05117.pdf]

2. MA : *MODEL ASSERTIONS FOR MONITORING AND IMPROVING ML MODELS*

   [https://cs.stanford.edu/~matei/papers/2020/mlsys_model_assertions.pdf]

3. Finding Lable and Model Errors in Perception Data with Learned Observation Assertions

   [https://arxiv.org/pdf/2201.05797.pdf]

## PUL

1. Learning From Positive and Unlabeled Data: A Survey

   [https://arxiv.org/pdf/1811.04820.pdf]

1. Convex Formulation for Learning from Positive and Unlabeled Data

   [http://proceedings.mlr.press/v37/plessis15.pdf]

1. Class-prior Estimation for Learning from Positive and Unlabeled Data

   [http://proceedings.mlr.press/v45/Christoffel15.pdf]

2. uPU : *Analysis of Learning from Positive and Unlabeled Data*

   [https://proceedings.neurips.cc/paper/2014/file/35051070e572e47d2c26c241ab88307f-Paper.pdf]

2. nnPU : *Positive-unlabeled Learning with Non-Negative Risk Estimator*

   [https://arxiv.org/pdf/1703.00593.pdf]

3. PUSB : *Learning from Positive and Unlabeled Data with a Selection Bias*

   [https://openreview.net/pdf?id=rJzLciCqKm]

4. Self-PU : *Self Boosted and Calibrated Positive-Unlabeled Training*

   [https://arxiv.org/pdf/2006.11280v1.pdf]

5. PULNS : *Positive-Unlabeled Learning with Effective Negative Sample Selector*

   [https://ojs.aaai.org/index.php/AAAI/article/view/17064/16871]

6. Positive-Unlabeled Learning from Imbalanced Data

   [https://www.ijcai.org/proceedings/2021/0412.pdf]

7. Dist-PU : *Positive-Unlabeled Learning from a Label Distribution Perspective*

   [https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Dist-PU_Positive-Unlabeled_Learning_From_a_Label_Distribution_Perspective_CVPR_2022_paper.pdf]

## ER

1. DADER : *Domain Adaptation for Deep Entity Resolution*

   [http://da.qcri.org/ntang/pubs/dader.pdf]

## OTHER

1. *The YouTube Video Recommendation System*

   [http://minegrado.ovh:3000/General-Team/docs/raw/branch/master/ML_DL_NN_books/Machine_Learning/Generic-books-1/The_YouTube_video_recommendation_system.pdf]

2. *Wide & Deep Learning for Recommender Systems*

   [https://dl.acm.org/doi/pdf/10.1145/2988450.2988454]

3. *What You Corrupt Is Not What You Crash : Challenges in Fuzzing Embedded Devices*

   [https://wcventure.github.io/FuzzingPaper/Paper/NDSS18_Muench_paper.pdf]
